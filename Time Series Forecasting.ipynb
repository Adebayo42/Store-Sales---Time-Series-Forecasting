{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9162b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install the darts library\n",
    "# %pip install darts\n",
    "\n",
    "# import neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from darts.utils.timeseries_generation import holidays_timeseries, TimeSeries, datetime_attribute_timeseries\n",
    "from darts.dataprocessing.pipeline import Pipeline\n",
    "from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, InvertibleMapper\n",
    "from darts.models import RandomForest\n",
    "from darts.metrics import mae\n",
    "from darts import TimeSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0287cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]', freq=None)\n",
      "Train range: 2013-01-01:2017-08-15\n",
      "Test range: 2017-08-16:2017-08-31\n",
      "Date Range: 1688\n",
      "Actual dates: 1684\n"
     ]
    }
   ],
   "source": [
    "### Data Preprocessing and Feature Engineering\n",
    "\n",
    "\n",
    "## Load the datasets\n",
    "train = pd.read_csv(\"./train.csv\",parse_dates=[\"date\"])\n",
    "test = pd.read_csv(\"./test.csv\", parse_dates=[\"date\"])\n",
    "oil = pd.read_csv(\"./oil.csv\", parse_dates=[\"date\"]).rename(columns={\"dcoilwtico\":\"oil\"})\n",
    "stores = pd.read_csv(\"./stores.csv\")\n",
    "transaction = pd.read_csv(\"./transactions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "## create function to reduce the memory usage\n",
    "def reduce_memory(df):\n",
    "    for col in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\" if df[col].dtype == \"float64\" else \"integer\")\n",
    "    return df\n",
    "\n",
    "train = reduce_memory(train)\n",
    "test = reduce_memory(test)\n",
    "oil = reduce_memory(oil)\n",
    "stores = reduce_memory(stores)\n",
    "transaction = reduce_memory(transaction)\n",
    "\n",
    "\n",
    "# holiday = pd.read_csv(\"./holidays_events.csv\", parse_dates=[\"date\"])\n",
    "date_rnge = pd.date_range(start=train[\"date\"].min(), end=train[\"date\"].max())\n",
    "train_dt = set(train[\"date\"])\n",
    "print(date_rnge.difference(train_dt))\n",
    "\n",
    "# set(date_rnge)-train_dt\n",
    "train_dt_max = train.date.max().date()\n",
    "train_dt_min = train.date.min().date()\n",
    "test_dt_max = test.date.max().date()\n",
    "test_dt_min = test.date.min().date()\n",
    "\n",
    "# print(f\"{train_dt_max - train_dt_min}\")\n",
    "print(f\"Train range: {train_dt_min}:{train_dt_max}\\n\"\n",
    "    f\"Test range: {test_dt_min}:{test_dt_max}\")\n",
    "print(f\"Date Range: {(train_dt_max - train_dt_min).days + 1}\")\n",
    "print(f\"Actual dates: {train.date.nunique()}\")\n",
    "\n",
    "## fix the missing dates\n",
    "full_date = pd.date_range(start=train[\"date\"].min(), end=train[\"date\"].max())\n",
    "\n",
    "new_index = pd.MultiIndex.from_product([full_date, train[\"store_nbr\"].unique(), train[\"family\"].unique()],\n",
    "                                       names=[\"date\", \"store_nbr\",\"family\"])\n",
    "\n",
    "train_new = train.set_index([\"date\",\"store_nbr\",\"family\"]).reindex(new_index).reset_index()\n",
    "\n",
    "### Fill the sales and promotion with zero values - no sales on those days\n",
    "train_new[[\"sales\", \"onpromotion\"]] = train_new[[\"sales\", \"onpromotion\"]].fillna(0)\n",
    "\n",
    "### fill the missing values in id column\n",
    "train_new[[\"id\"]] = pd.DataFrame(pd.RangeIndex(start=0.0, step=1, stop=len(train_new)).astype(\"float\"))\n",
    "\n",
    "## Oil dataset preprocessing\n",
    "max_dt_oil, min_dt_oil = oil[\"date\"].max(), oil[\"date\"].min()\n",
    "oil_dt_range = pd.DataFrame({\"date\":pd.date_range(start=min_dt_oil, end=max_dt_oil)})\n",
    "oil_new = oil_dt_range.merge(oil, on=\"date\", how=\"left\").interpolate(method=\"linear\",limit_direction=\"both\").round(2)\n",
    "oil_new[\"ma_7\"]=oil_new[\"oil\"].rolling(window=7, min_periods=1).mean().round(2)\n",
    "oil_new[\"ma_14\"]=oil_new[\"oil\"].rolling(window=14, min_periods=1).mean().round(2)\n",
    "oil_new[\"ma_21\"]=oil_new[\"oil\"].rolling(window=21, min_periods=1).mean().round(2)\n",
    "oil_new[\"ma_28\"]=oil_new[\"oil\"].rolling(window=28, min_periods=1).mean().round(2)\n",
    "\n",
    "## Create series\n",
    "oil_series = TimeSeries.from_dataframe(oil_new, time_col=\"date\", value_cols=\"oil\")\n",
    "oil_series_ma_7 = TimeSeries.from_dataframe(oil_new, time_col=\"date\", value_cols=\"ma_7\")\n",
    "oil_series_ma_14 = TimeSeries.from_dataframe(oil_new, time_col=\"date\", value_cols=\"ma_14\")\n",
    "oil_series_ma_21 = TimeSeries.from_dataframe(oil_new, time_col=\"date\", value_cols=\"ma_21\")\n",
    "oil_series_ma_28 = TimeSeries.from_dataframe(oil_new, time_col=\"date\", value_cols=\"ma_28\")\n",
    "oil_series = oil_series.stack(oil_series_ma_7).stack(oil_series_ma_14).stack(oil_series_ma_21).stack(oil_series_ma_28)\n",
    "\n",
    "## create holidat series\n",
    "holiday_Ecuador = holidays_timeseries(oil_series, country_code=\"EC\")\n",
    "\n",
    "# transaction preprocessing = handle missing values\n",
    "\n",
    "# group the train dataset sales and and assign transaction to be 0 where sales is 0\n",
    "store_sales_agg = train_new.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index().round(2)\n",
    "transact = transaction.merge(store_sales_agg, on=[\"date\", \"store_nbr\"], how=\"right\")\n",
    "transact.loc[transact[\"sales\"] ==0,\"transactions\"] = 0\n",
    "transaction = transact.drop(columns=\"sales\").sort_values([\"store_nbr\",\"date\"],ignore_index=True)\n",
    "\n",
    "## Interpolate the remaining missing dataset\n",
    "transaction[\"transactions\"] = (transaction\n",
    "                               .groupby(\"store_nbr\", group_keys=False)\n",
    "                               .transactions.apply(lambda x: x.interpolate(method=\"linear\", \n",
    "                                                                           limit_direction=\"both\")))\n",
    "\n",
    "## Create transaction series\n",
    "transaction_series = TimeSeries.from_group_dataframe(transaction,\n",
    "                                              group_cols=[\"store_nbr\"], \n",
    "                                              time_col=\"date\",\n",
    "                                              value_cols=\"transactions\")\n",
    "\n",
    "\n",
    "### merge the datasets\n",
    "\n",
    "train_data = (train_new\n",
    "              .merge(stores, on=\"store_nbr\",\n",
    "                     how=\"left\").drop(columns=[\"id\"]))\n",
    "\n",
    "\n",
    "### Generate date_time attributes\n",
    "day_attrb = datetime_attribute_timeseries(oil_series,attribute=\"day\")\n",
    "week_attrb = datetime_attribute_timeseries(oil_series,attribute=\"week\")\n",
    "month_attrb = datetime_attribute_timeseries(oil_series,attribute=\"month\")\n",
    "quarter_attrb = datetime_attribute_timeseries(oil_series,attribute=\"quarter\")\n",
    "year_attrb = datetime_attribute_timeseries(oil_series,attribute=\"year\")\n",
    "dayofweek_attrb = datetime_attribute_timeseries(oil_series,attribute=\"dayofweek\")\n",
    "dayofweek_cyc_attrb = datetime_attribute_timeseries(oil_series,attribute=\"dayofweek\",cyclic=True)\n",
    "dayofyear_attrb = datetime_attribute_timeseries(oil_series,attribute=\"dayofyear\")\n",
    "dayofyear_cyc_attrb = datetime_attribute_timeseries(oil_series,attribute=\"dayofyear\", cyclic=True)\n",
    "week_cyc_attrb = datetime_attribute_timeseries(oil_series,attribute=\"week\", cyclic=True)\n",
    "monthend_attrb = datetime_attribute_timeseries(oil_series,attribute=\"is_month_end\")\n",
    "\n",
    "## Payday Flag attributes\n",
    "pay_day_flag = ((oil_series.time_index.day ==15) | (oil_series.time_index.is_month_end)).astype(\"int\")\n",
    "payday_attrb = TimeSeries.from_times_and_values(times=oil_series.time_index,values=pay_day_flag.reshape(-1,1), columns=[\"paydayflag\"])\n",
    "\n",
    "## Earthquake flag attributes\n",
    "eart_q_start_dt = pd.to_datetime(\"2016-04-16\")\n",
    "eart_q_end_dt = eart_q_start_dt + pd.to_timedelta(4, unit=\"W\") ## 4weeeks of impact\n",
    "earth_q_flag = ((oil_series.time_index >=eart_q_start_dt) & (oil_series.time_index <= eart_q_end_dt)).astype(\"int\")\n",
    "earth_attrb = TimeSeries.from_times_and_values(times=oil_series.time_index,values=earth_q_flag.reshape(-1,1),columns=[\"earthquakeflag\"])\n",
    "\n",
    "## Stack all dates attributes\n",
    "date_attributes = (day_attrb.stack(week_attrb).stack(month_attrb).\n",
    "                   stack(quarter_attrb).stack(year_attrb)\n",
    "                   .stack(dayofweek_attrb).stack(dayofweek_cyc_attrb)\n",
    "                   .stack(dayofyear_attrb).stack(dayofyear_cyc_attrb)\n",
    "                   .stack(week_cyc_attrb).stack(monthend_attrb)\n",
    "                   .stack(payday_attrb).stack(earth_attrb))\n",
    "\n",
    "## transaction -- past covariates\n",
    "## oil, date_attributes, holiday --- future covariates\n",
    "## onpromotion, city, state, type, cluster -- static covariates\n",
    "\n",
    "## scale the transaction series using MixMax\n",
    "transaction_scaled = []\n",
    "for ts in transaction_series:\n",
    "    scaler = Scaler()\n",
    "    ts_scaled = scaler.fit_transform(ts)\n",
    "    transaction_scaled.append(ts_scaled)\n",
    "    \n",
    "## scale the oil series using MixMax\n",
    "oil_scaler = Scaler()\n",
    "oil_scaled = oil_scaler.fit_transform(oil_series)\n",
    "\n",
    "## scale the date_attributes series using MixMax\n",
    "date_attribute_scaler = Scaler()\n",
    "date_attribute_scaled = date_attribute_scaler.fit_transform(date_attributes)\n",
    "\n",
    "## scale the holiday series using MixMax\n",
    "holiday_scaler = Scaler()\n",
    "holiday_scaled = holiday_scaler.fit_transform(holiday_Ecuador)\n",
    "\n",
    "## future covariates\n",
    "future_cov = oil_scaled.stack(date_attribute_scaled).stack(holiday_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d93bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Global Model for Family: SEAFOOD\n",
      "Family SEAFOOD - Final MAE: 1.5372\n"
     ]
    }
   ],
   "source": [
    "  ## Train Global Model by Family training on a multiple series per family\n",
    "\n",
    "forecast_horizon = 16\n",
    "\n",
    "rf_models = {}\n",
    "rf_forecasts = {}\n",
    "rf_errors = {}\n",
    "ts_family_scaler ={}\n",
    "\n",
    "import gc\n",
    "\n",
    "for family in train_data[\"family\"].unique()[32:]:\n",
    "    print(f\"\\nTraining Global Model for Family: {family}\")\n",
    "    \n",
    "    # Build and scale only one family series at a time\n",
    "    df = train_data[train_data[\"family\"] == family].copy()\n",
    "    series_list = TimeSeries.from_group_dataframe(df=df,\n",
    "                                                  group_cols=[\"store_nbr\"],\n",
    "                                                  time_col=\"date\",\n",
    "                                                  value_cols=\"sales\",\n",
    "                                                  static_cols=[\"onpromotion\",\"city\",\"type\",\"cluster\"])\n",
    "\n",
    "    static_scaler = StaticCovariatesTransformer()\n",
    "    target_scaler_log = InvertibleMapper(fn=np.log1p, inverse_fn=np.expm1)\n",
    "    target_scaler_minmax = Scaler()\n",
    "    pipeline = Pipeline([static_scaler, target_scaler_log, target_scaler_minmax])\n",
    "    series_list_scaled = pipeline.fit_transform(series_list)\n",
    "\n",
    "    # Keep scalers if needed\n",
    "    ts_family_scaler[family] = pipeline\n",
    "\n",
    "    # Get past/future covariates\n",
    "    past_cov_list = transaction_scaled[:len(series_list)]\n",
    "    future_cov_list = [future_cov.slice(series.start_time(), series.end_time() + forecast_horizon * series.freq) for series in series_list]\n",
    "\n",
    "    model = RandomForest(\n",
    "        lags=7,\n",
    "        lags_past_covariates=7,\n",
    "        lags_future_covariates=(0, forecast_horizon),\n",
    "        output_chunk_length=forecast_horizon,\n",
    "        n_estimators=100,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(series=series_list_scaled,\n",
    "              past_covariates=past_cov_list,\n",
    "              future_covariates=future_cov_list)\n",
    "\n",
    "    forecasts = model.predict(n=forecast_horizon,\n",
    "                              series=series_list_scaled,\n",
    "                              past_covariates=past_cov_list,\n",
    "                              future_covariates=future_cov_list)\n",
    "\n",
    "    forecasts_unscaled = pipeline.inverse_transform(forecasts)\n",
    "    targets_unscaled = pipeline.inverse_transform(series_list_scaled)\n",
    "\n",
    "    actuals = [ts[-forecast_horizon:] for ts in targets_unscaled]\n",
    "\n",
    "    backtest = model.historical_forecasts(\n",
    "        series=series_list_scaled,\n",
    "        past_covariates=past_cov_list,\n",
    "        future_covariates=future_cov_list,\n",
    "        start=-forecast_horizon,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        stride=forecast_horizon,\n",
    "        retrain=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    backtest_unscaled = pipeline.inverse_transform(backtest)\n",
    "    mae_score = mae(actuals, backtest_unscaled)\n",
    "    mean_mae = np.mean(mae_score)\n",
    "\n",
    "    print(f\"Family {family} - Final MAE: {mean_mae:.4f}\")\n",
    "\n",
    "    # Store minimal results\n",
    "    rf_models[family] = model\n",
    "    rf_forecasts[family] = forecasts_unscaled\n",
    "    rf_errors[family] = mean_mae\n",
    "\n",
    "    # Clear memory\n",
    "    del series_list, series_list_scaled, forecasts, forecasts_unscaled, targets_unscaled\n",
    "    del past_cov_list, future_cov_list, backtest, backtest_unscaled, actuals\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a91c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   family  store_nbr      sales\n",
      "0 2017-08-16  SEAFOOD        1.0  21.103609\n",
      "1 2017-08-17  SEAFOOD        1.0  21.539042\n",
      "2 2017-08-18  SEAFOOD        1.0  21.040856\n",
      "3 2017-08-19  SEAFOOD        1.0  18.032994\n",
      "4 2017-08-20  SEAFOOD        1.0  14.106516\n"
     ]
    }
   ],
   "source": [
    "# Combine all forecast results into one DataFrame\n",
    "forecast_dfs = []\n",
    "\n",
    "for family, forecast_list in rf_forecasts.items():\n",
    "    for i, ts in enumerate(forecast_list):\n",
    "        store_nbr = ts.static_covariates[\"store_nbr\"].iloc[0]\n",
    "        df = ts.to_dataframe().reset_index()\n",
    "        df.columns = [\"date\", \"sales\"]\n",
    "        df[\"family\"] = family\n",
    "        df[\"store_nbr\"] = store_nbr\n",
    "        forecast_dfs.append(df)\n",
    "\n",
    "# Final forecast DataFrame\n",
    "forecast_df_final = pd.concat(forecast_dfs, ignore_index=True)\n",
    "forecast_df_final = forecast_df_final[[\"date\", \"family\", \"store_nbr\", \"sales\"]]\n",
    "forecast_df_final = forecast_df_final.sort_values(by=[\"family\", \"store_nbr\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# View output\n",
    "print(forecast_df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evalute the Average Mean Absolute error of the models\n",
    "key = []\n",
    "val = []\n",
    "for k, v in rf_errors.items():\n",
    "    key.append(k)\n",
    "    val.append(v)\n",
    "    \n",
    "df_error = pd.DataFrame({\"family\":key, \n",
    "                         \"error\":val })\n",
    "\n",
    "print(f\"The Average Mean Absolute Error is: {df_error[\"error\"].mean()}\") ## mean absolute error 22.76573340747008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e130cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Family: SEAFOOD\n",
      "  Training Model for Store: 1\n",
      "    Store 1 - MAE: 0.1968\n",
      "  Training Model for Store: 10\n",
      "    Store 10 - MAE: 4.6539\n",
      "  Training Model for Store: 11\n",
      "    Store 11 - MAE: 1.1523\n",
      "  Training Model for Store: 12\n",
      "    Store 12 - MAE: 0.6396\n",
      "  Training Model for Store: 13\n",
      "    Store 13 - MAE: 0.0232\n",
      "  Training Model for Store: 14\n",
      "    Store 14 - MAE: 1.4217\n",
      "  Training Model for Store: 15\n",
      "    Store 15 - MAE: 0.1459\n",
      "  Training Model for Store: 16\n",
      "    Store 16 - MAE: 0.2725\n",
      "  Training Model for Store: 17\n",
      "    Store 17 - MAE: 0.5112\n",
      "  Training Model for Store: 18\n",
      "    Store 18 - MAE: 4.1977\n",
      "  Training Model for Store: 19\n",
      "    Store 19 - MAE: 3.8115\n",
      "  Training Model for Store: 2\n",
      "    Store 2 - MAE: 0.5610\n",
      "  Training Model for Store: 20\n",
      "    Store 20 - MAE: 0.9530\n",
      "  Training Model for Store: 21\n",
      "    Store 21 - MAE: 0.1757\n",
      "  Training Model for Store: 22\n",
      "    Store 22 - MAE: 0.3055\n",
      "  Training Model for Store: 23\n",
      "    Store 23 - MAE: 1.2621\n",
      "  Training Model for Store: 24\n",
      "    Store 24 - MAE: 4.1787\n",
      "  Training Model for Store: 25\n",
      "    Store 25 - MAE: 0.5928\n",
      "  Training Model for Store: 26\n",
      "    Store 26 - MAE: 0.2467\n",
      "  Training Model for Store: 27\n",
      "    Store 27 - MAE: 2.8176\n",
      "  Training Model for Store: 28\n",
      "    Store 28 - MAE: 3.4881\n",
      "  Training Model for Store: 29\n",
      "    Store 29 - MAE: 0.7869\n",
      "  Training Model for Store: 3\n",
      "    Store 3 - MAE: 1.7843\n",
      "  Training Model for Store: 30\n",
      "    Store 30 - MAE: 0.6227\n",
      "  Training Model for Store: 31\n",
      "    Store 31 - MAE: 0.4053\n",
      "  Training Model for Store: 32\n",
      "    Store 32 - MAE: 0.1406\n",
      "  Training Model for Store: 33\n",
      "    Store 33 - MAE: 0.1456\n",
      "  Training Model for Store: 34\n",
      "    Store 34 - MAE: 0.2537\n",
      "  Training Model for Store: 35\n",
      "    Store 35 - MAE: 1.2312\n",
      "  Training Model for Store: 36\n",
      "    Store 36 - MAE: 0.3233\n",
      "  Training Model for Store: 37\n",
      "    Store 37 - MAE: 0.4813\n",
      "  Training Model for Store: 38\n",
      "    Store 38 - MAE: 0.1222\n",
      "  Training Model for Store: 39\n",
      "    Store 39 - MAE: 0.1399\n",
      "  Training Model for Store: 4\n",
      "    Store 4 - MAE: 1.4332\n",
      "  Training Model for Store: 40\n",
      "    Store 40 - MAE: 0.5678\n",
      "  Training Model for Store: 41\n",
      "    Store 41 - MAE: 1.4663\n",
      "  Training Model for Store: 42\n",
      "    Store 42 - MAE: 0.8024\n",
      "  Training Model for Store: 43\n",
      "    Store 43 - MAE: 0.2220\n",
      "  Training Model for Store: 44\n",
      "    Store 44 - MAE: 1.6982\n",
      "  Training Model for Store: 45\n",
      "    Store 45 - MAE: 4.5409\n",
      "  Training Model for Store: 46\n",
      "    Store 46 - MAE: 2.6160\n",
      "  Training Model for Store: 47\n",
      "    Store 47 - MAE: 0.1888\n",
      "  Training Model for Store: 48\n",
      "    Store 48 - MAE: 0.2220\n",
      "  Training Model for Store: 49\n",
      "    Store 49 - MAE: 3.7008\n",
      "  Training Model for Store: 5\n",
      "    Store 5 - MAE: 0.4119\n",
      "  Training Model for Store: 50\n",
      "    Store 50 - MAE: 0.7566\n",
      "  Training Model for Store: 51\n",
      "    Store 51 - MAE: 6.7545\n",
      "  Training Model for Store: 52\n",
      "    Store 52 - MAE: 0.3405\n",
      "  Training Model for Store: 53\n",
      "    Store 53 - MAE: 0.3052\n",
      "  Training Model for Store: 54\n",
      "    Store 54 - MAE: 0.0579\n",
      "  Training Model for Store: 6\n",
      "    Store 6 - MAE: 4.8730\n",
      "  Training Model for Store: 7\n",
      "    Store 7 - MAE: 0.7978\n",
      "  Training Model for Store: 8\n",
      "    Store 8 - MAE: 2.1170\n",
      "  Training Model for Store: 9\n",
      "    Store 9 - MAE: 0.3638\n"
     ]
    }
   ],
   "source": [
    "## Train a Global Model on individual store serie in each family\n",
    "\n",
    "\n",
    "## Scale the transaction series   \n",
    "transaction_scaled = {}\n",
    "for ts in transaction_series:\n",
    "    store_id = ts.static_covariates[\"store_nbr\"].item() if \"store_nbr\" in ts.static_covariates else ts.components[0][1]\n",
    "    scaler = Scaler()\n",
    "    ts_scaled = scaler.fit_transform(ts)\n",
    "    transaction_scaled[store_id] = ts_scaled\n",
    "\n",
    "# set the forcast horizon\n",
    "forecast_horizon = 16\n",
    "\n",
    "\n",
    "# rf_models = {}\n",
    "rf_forecasts = {}\n",
    "rf_errors = {}\n",
    "ts_series_scaler = {}\n",
    "\n",
    "# Loop over each family first\n",
    "for family in train_data[\"family\"].unique()[32:]:\n",
    "    print(f\"\\nProcessing Family: {family}\")\n",
    "    \n",
    "    df_family = train_data[train_data[\"family\"] == family].copy()\n",
    "    \n",
    "    # Loop over each store in this family\n",
    "    for idx, store in enumerate(df_family[\"store_nbr\"].unique()):\n",
    "        print(f\"  Training Model for Store: {store}\")\n",
    "        \n",
    "        \n",
    "        df = df_family[df_family[\"store_nbr\"] == store].copy()\n",
    "        \n",
    "        # Convert single series dataframe to TimeSeries object\n",
    "        series = TimeSeries.from_dataframe(\n",
    "            df,\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"sales\",\n",
    "            static_covariates=df[[\"onpromotion\",\"city\",\"type\",\"cluster\"]].iloc[0]\n",
    "        )\n",
    "        \n",
    "        # Build and fit scaler pipeline per series\n",
    "        static_scaler = StaticCovariatesTransformer()\n",
    "        target_scaler_log = InvertibleMapper(fn=np.log1p, inverse_fn=np.expm1)\n",
    "        target_scaler_minmax = Scaler()\n",
    "        pipeline = Pipeline([static_scaler, target_scaler_log, target_scaler_minmax])\n",
    "        \n",
    "        series_scaled = pipeline.fit_transform(series)\n",
    "        \n",
    "        ts_series_scaler[(family, store)] = pipeline\n",
    "        \n",
    "        # Prepare covariates (make sure these are prepared for this single series)\n",
    "        past_cov = transaction_scaled[store]\n",
    "        ## future covariates\n",
    "        future_cov = oil_scaled.stack(date_attribute_scaled).stack(holiday_scaled)\n",
    "\n",
    "        \n",
    "        ## Instantiate the model\n",
    "        model = RandomForest(\n",
    "            lags=7,\n",
    "            lags_past_covariates=7,\n",
    "            lags_future_covariates=(0, forecast_horizon),\n",
    "            output_chunk_length=forecast_horizon,\n",
    "            n_estimators=100,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            series=series_scaled,\n",
    "            past_covariates=past_cov,\n",
    "            future_covariates=future_cov\n",
    "        )\n",
    "        \n",
    "        forecasts = model.predict(\n",
    "            n=forecast_horizon,\n",
    "            series=series_scaled,\n",
    "            past_covariates=past_cov,\n",
    "            future_covariates=future_cov\n",
    "        )\n",
    "        \n",
    "        forecasts_unscaled = pipeline.inverse_transform(forecasts)\n",
    "        target_unscaled = pipeline.inverse_transform(series_scaled)\n",
    "        \n",
    "        actual = target_unscaled[-forecast_horizon:]\n",
    "        \n",
    "        ## Backtest to evaluate model performance\n",
    "        backtest = model.historical_forecasts(\n",
    "            series=series_scaled,\n",
    "            past_covariates=past_cov,\n",
    "            future_covariates=future_cov,\n",
    "            start=-forecast_horizon,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            stride=forecast_horizon,\n",
    "            retrain=False,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        backtest_unscaled = pipeline.inverse_transform(backtest)\n",
    "        mae_score = mae([actual], [backtest_unscaled])\n",
    "        mean_mae = np.mean(mae_score)\n",
    "        \n",
    "        print(f\"    Store {store} - MAE: {mean_mae:.4f}\")\n",
    "        \n",
    "        # Store results keyed by (family, store)\n",
    "        # rf_models[(family, store)] = model\n",
    "        rf_forecasts[(family, store)] = forecasts_unscaled\n",
    "        rf_errors[(family, store)] = mean_mae\n",
    "        \n",
    "        # Cleanup\n",
    "        del series, series_scaled, forecasts, forecasts_unscaled, target_unscaled\n",
    "        del past_cov, future_cov, backtest, backtest_unscaled, actual\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch the forcast for all family and store_nbr\n",
    "\n",
    "forecast_records = []\n",
    "\n",
    "for (family, store), ts in rf_forecasts.items():\n",
    "    df = ts.to_dataframe().reset_index()  \n",
    "    df.columns = ['date', 'sales']\n",
    "    df['family'] = family\n",
    "    df['store_nbr'] = store\n",
    "    forecast_records.append(df)\n",
    "\n",
    "# Combine all forecasts into a single DataFrame\n",
    "forecast_df = pd.concat(forecast_records, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evalute the Average Mean Absolute error of the models\n",
    "fam_forcast = []\n",
    "str_forcast = []\n",
    "error_for = []\n",
    "for (family_forcast_v1, store_nbr), error_forcast_v1 in rf_errors.items():\n",
    "    fam_forcast.append(family_forcast_v1)\n",
    "    str_forcast.append(store_nbr)\n",
    "    error_for.append(error_forcast_v1)\n",
    "    \n",
    "eror_df = pd.DataFrame({\"family\": fam_forcast,\n",
    "                        \"store_nbr\": str_forcast,\n",
    "                        \"error\": error_for})\n",
    "\n",
    "print(f\"The Average Mean Absolute Error is: {eror_df[\"error\"].mean()}\") ## mean absolute error 18.465658008191983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b32d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Submission by stacking the model\n",
    "\n",
    "final_submission = forecast_df_final.merge(forecast_df, on=['date', 'family','store_nbr'], how=\"left\")\n",
    "final_submission[\"sales\"] = (final_submission[\"sales_x\"] + final_submission[\"sales_y\"])/2\n",
    "final_submission.drop(columns=[\"sales_x\", \"sales_y\"], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
